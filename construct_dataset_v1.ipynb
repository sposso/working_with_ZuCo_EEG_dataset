{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34731cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This code is based on the code from this repository: https://github.com/MikeWangWZHL/EEG-To-Text\"\"\"\n",
    "\n",
    "import scipy.io as io\n",
    "import h5py\n",
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2de835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current working directory\n",
    "home_directory = os.getcwd()\n",
    "# directory of the data \n",
    "data_directory = '/home/sposso22/Documents/datasets/ZuCo'\n",
    "\n",
    "\n",
    "# Tasks\n",
    "tasks = ['task1-SR', 'task2-NR', 'task3-TSR']\n",
    "\n",
    "task = tasks[0]\n",
    "\n",
    "\n",
    "\n",
    "# Load the Matlab file\n",
    "input_mat_files_dir = os.path.join(data_directory, f'{task}/Matlab_files')\n",
    "\n",
    "# Create output folder to save EEG and eye-tracking data as pickle files\n",
    "output_file_folder = os.path.join(home_directory, f'{task}_pickle')\n",
    "\n",
    "os.makedirs(output_file_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366a1ccd",
   "metadata": {},
   "source": [
    "### Load info for each subject "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed72975",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_files = os.listdir(input_mat_files_dir)\n",
    "path_mat_files = [os.path.join(input_mat_files_dir,mat_file) for mat_file in mat_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801321f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whole dataset dictionary\n",
    "dataset_dict = {}\n",
    "\n",
    "for mat_file in tqdm(path_mat_files):\n",
    "\n",
    "    # get subject name from the file name\n",
    "    subject_name = os.path.basename(mat_file).split('.')[0].replace('results','').strip()\n",
    "     \n",
    "    # The subject data will be saved in a list\n",
    "    dataset_dict[subject_name] = []\n",
    "\n",
    "   \n",
    "\n",
    "    mat_data = io.loadmat(mat_file,squeeze_me=True,struct_as_record=False)['sentenceData']\n",
    "\n",
    "    \n",
    "     # Sentence level data\n",
    "    for sent in mat_data: \n",
    "\n",
    "        word_data = sent.word\n",
    "\n",
    "        if not isinstance(word_data, float):\n",
    "\n",
    "            # First key: sentence content\n",
    "            sent_obj = {'content': sent.content}\n",
    "            \n",
    "            # second key : Oscillatory in different power bands (Theta, Alpha, Beta, Gamma)\n",
    "            sent_obj['sentence_level_EEG'] = {'mean_t1':sent.mean_t1, 'mean_t2':sent.mean_t2, \n",
    "                                              'mean_a1':sent.mean_a1, 'mean_a2':sent.mean_a2, \n",
    "                                              'mean_b1':sent.mean_b1, 'mean_b2':sent.mean_b2, \n",
    "                                              'mean_g1':sent.mean_g1, 'mean_g2':sent.mean_g2}\n",
    "\n",
    "            if task == 'task1-SR':\n",
    "\n",
    "                # task1-SR: Read sentences, answer control questions\n",
    "\n",
    "                sent_obj['answer_EEG'] = {'answer_mean_t1':sent.answer_mean_t1, 'answer_mean_t2':sent.answer_mean_t2,\n",
    "                                         'answer_mean_a1':sent.answer_mean_a1,'answer_mean_a2':sent.answer_mean_a2, \n",
    "                                         'answer_mean_b1':sent.answer_mean_b1, 'answer_mean_b2':sent.answer_mean_b2, \n",
    "                                         'answer_mean_g1':sent.answer_mean_g1, 'answer_mean_g2':sent.answer_mean_g2}\n",
    "                                         \n",
    "\n",
    "            # world level data\n",
    "            sent_obj['word'] = []\n",
    "\n",
    "            # Features from eye-tracking \n",
    "            word_tokens_has_fixation =[]\n",
    "            word_tokens_with_mask = []\n",
    "            word_tokens_all = []\n",
    "\n",
    "            for word in word_data:\n",
    "                word_obj = {'content': word.content}\n",
    "                word_tokens_all.append(word.content)\n",
    "\n",
    "                word_obj['n_fixations'] = word.nFixations\n",
    "\n",
    "  \n",
    "                if isinstance(word.nFixations, (int, np.integer)) and word.nFixations > 0:\n",
    "\n",
    "                    print('word n fixations:', word.nFixations)\n",
    "                    print(type(word.nFixations))\n",
    "\n",
    "                    word_obj['word_level_EEG'] = {'FFD':{'FFD_t1':word.FFD_t1, 'FFD_t2':word.FFD_t2, \n",
    "                                                         'FFD_a1':word.FFD_a1, 'FFD_a2':word.FFD_a2, \n",
    "                                                         'FFD_b1':word.FFD_b1, 'FFD_b2':word.FFD_b2, \n",
    "                                                         'FFD_g1':word.FFD_g1, 'FFD_g2':word.FFD_g2}}\n",
    "\n",
    "                    word_obj['word_level_EEG']['TRT'] = {'TRT_t1':word.TRT_t1, 'TRT_t2':word.TRT_t2, \n",
    "                                                         'TRT_a1':word.TRT_a1, 'TRT_a2':word.TRT_a2, \n",
    "                                                         'TRT_b1':word.TRT_b1, 'TRT_b2':word.TRT_b2, \n",
    "                                                         'TRT_g1':word.TRT_g1, 'TRT_g2':word.TRT_g2}\n",
    "                    word_obj['word_level_EEG']['GD'] = {'GD_t1':word.GD_t1, 'GD_t2':word.GD_t2, \n",
    "                                                        'GD_a1':word.GD_a1, 'GD_a2':word.GD_a2, \n",
    "                                                        'GD_b1':word.GD_b1, 'GD_b2':word.GD_b2, \n",
    "                                                        'GD_g1':word.GD_g1, 'GD_g2':word.GD_g2}\n",
    "                    sent_obj['word'].append(word_obj)\n",
    "                    word_tokens_has_fixation.append(word.content)\n",
    "                    word_tokens_with_mask.append(word.content)\n",
    "\n",
    "                else:\n",
    "\n",
    "                    word_tokens_with_mask.append('[MASK]')\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "                    continue\n",
    "            sent_obj['word_tokens_has_fixation'] = word_tokens_has_fixation\n",
    "            sent_obj['word_tokens_with_mask'] = word_tokens_with_mask\n",
    "            sent_obj['word_tokens_all'] = word_tokens_all\n",
    "\n",
    "\n",
    "            dataset_dict[subject_name].append(sent_obj)\n",
    "\n",
    "\n",
    "\n",
    "# Save the dataset dictionary as a pickle file\n",
    "output_name = f'{task}_dataset_dict_v1.pkl'\n",
    "\n",
    "with open(os.path.join(output_file_folder,output_name), 'wb') as handle:\n",
    "    pickle.dump(dataset_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0146798d",
   "metadata": {},
   "source": [
    "## Sanity check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad7bd595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subjects: dict_keys(['ZKB_SR', 'ZJS_SR', 'ZKH_SR', 'ZJM_SR', 'ZAB_SR', 'ZPH_SR', 'ZKW_SR', 'ZMG_SR', 'ZDN_SR', 'ZJN_SR', 'ZDM_SR', 'ZGW_SR'])\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(output_file_folder,output_name), 'rb') as handle:\n",
    "    whole_dataset = pickle.load(handle)\n",
    "print('subjects:', whole_dataset.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ad562b",
   "metadata": {},
   "source": [
    "### Let's explore the data for the first subject and validate the gathered information above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecb77e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences for task 1 (SR): 400\n"
     ]
    }
   ],
   "source": [
    "subject_1 = list(whole_dataset.keys())[0]\n",
    "subject_1_data = whole_dataset[subject_1]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "The first task (SR) correspond to the sentiment reading from the Stanford Sentiment Treebank dataset. 400 sentences were presented\n",
    "to the subjects. Each sentence is annotated with a sentiment label (positive, negative, neutral). \n",
    "\n",
    "\"\"\" \n",
    "print(\"Number of sentences for task 1 (SR):\", len(subject_1_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60313faf",
   "metadata": {},
   "source": [
    "#### Let's go deeper into the info stored for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0e6d46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The info for first sentence is stored in : <class 'dict'>\n",
      "dict_keys(['content', 'sentence_level_EEG', 'answer_EEG', 'word', 'word_tokens_has_fixation', 'word_tokens_with_mask', 'word_tokens_all'])\n"
     ]
    }
   ],
   "source": [
    "first_sentence_info = subject_1_data[0]\n",
    "print('The info for first sentence is stored in :', type(first_sentence_info))\n",
    "print(first_sentence_info.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04ed17e",
   "metadata": {},
   "source": [
    "### Sentence read by the subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9dd631a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The content of the first sentence is the following sentence : Presents a good case while failing to provide a reason for us to care beyond the very basic dictums of human decency.\n"
     ]
    }
   ],
   "source": [
    "content = first_sentence_info['content']\n",
    "print('The content of the first sentence is the following sentence :', content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97eb6d0",
   "metadata": {},
   "source": [
    "### EEG features per sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfc0af34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence level EEG features are stored in : <class 'dict'>\n",
      "dict_keys(['mean_t1', 'mean_t2', 'mean_a1', 'mean_a2', 'mean_b1', 'mean_b2', 'mean_g1', 'mean_g2'])\n"
     ]
    }
   ],
   "source": [
    "sentence_level_EEG = first_sentence_info['sentence_level_EEG']\n",
    "print('The sentence level EEG features are stored in :', type(sentence_level_EEG))\n",
    "\n",
    "\"\"\"\n",
    "From the ZuCo paper:\n",
    "\n",
    "They extracted EEG features based on the sentence-level by calculating the power in each frequency band.\n",
    "For all the EEG recorded while a subject reads one sentence, they compute the average power per band\n",
    "\n",
    "\"\"\" \n",
    "print(sentence_level_EEG.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1760cdd1",
   "metadata": {},
   "source": [
    "### EEG Band Power Feature Extraction \n",
    "\n",
    "1. **Band-pass filter the EEG**  \n",
    "   Each EEG channel is filtered into frequency bands (theta, alpha, beta, gamma).  \n",
    "\n",
    "2. **Hilbert transform**  \n",
    "   For each band-limited signal $x_b(t)$, apply the Hilbert transform:\n",
    "\n",
    "   \n",
    "   $$z_b(t) = x_b(t) + i \\, H(x_b(t))$$\n",
    "\n",
    "\n",
    "3. **Amplitude envelope**  \n",
    "   Compute the instantaneous amplitude:\n",
    "\n",
    "   \n",
    "   $$ A_b(t) = |z_b(t)| = \\sqrt{x_b(t)^2 + H(x_b(t))^2} $$\n",
    "   \n",
    "\n",
    "4. **Instantaneous power**  \n",
    "   Power is amplitude squared:\n",
    "\n",
    "   \n",
    "   $$P_b(t) = A_b(t)^2$$\n",
    "   \n",
    "\n",
    "5. **Average over a time window**  \n",
    "   For a fixation or sentence time window \\(W\\):\n",
    "\n",
    "   \n",
    "   $$\\overline{P}_b = \\frac{1}{|W|} \\sum_{t \\in W} P_b(t)$$\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c659d4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_t1 (105,)\n",
      "mean_t2 (105,)\n",
      "mean_a1 (105,)\n",
      "mean_a2 (105,)\n",
      "mean_b1 (105,)\n",
      "mean_b2 (105,)\n",
      "mean_g1 (105,)\n",
      "mean_g2 (105,)\n"
     ]
    }
   ],
   "source": [
    "# print shape of each frequency band\n",
    "\"\"\"\n",
    "105 EEG channels were used for scalp recordings\n",
    "\"\"\"\n",
    "\n",
    "for band in sentence_level_EEG.keys():\n",
    "    print(band, np.shape(sentence_level_EEG[band]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14a1de5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer EEG features are stored in : <class 'dict'>\n",
      "dict_keys(['answer_mean_t1', 'answer_mean_t2', 'answer_mean_a1', 'answer_mean_a2', 'answer_mean_b1', 'answer_mean_b2', 'answer_mean_g1', 'answer_mean_g2'])\n"
     ]
    }
   ],
   "source": [
    "## Anser EEG features for task 1 (SR)\n",
    "\n",
    "answer_EEG = first_sentence_info['answer_EEG']\n",
    "print('The answer EEG features are stored in :', type(answer_EEG))\n",
    "print(answer_EEG.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37392529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words in the first sentence that has fixations is : 18\n",
      "The words that have fixations are : ['Presents', 'a', 'good', 'case', 'while', 'failing', 'to', 'provide', 'a', 'reason', 'for', 'to', 'care', 'beyond', 'very', 'basic', 'dictums', 'human']\n"
     ]
    }
   ],
   "source": [
    "## Only words with fixations are stored, so their number is smaller than the total words in the sentence because some words do not have fixations\n",
    "print('The number of words in the first sentence that has fixations is :', len(first_sentence_info['word_tokens_has_fixation']))\n",
    "print('The words that have fixations are :', first_sentence_info['word_tokens_has_fixation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf47ae8",
   "metadata": {},
   "source": [
    "## Word level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "25f81a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word level data is stored in : <class 'list'>\n",
      "The number of words in the first sentence is : 18\n"
     ]
    }
   ],
   "source": [
    "### word level data\n",
    "word_data = first_sentence_info['word']\n",
    "print('The word level data is stored in :', type(word_data))\n",
    "print('The number of words in the first sentence is :', len(word_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5b7d0a",
   "metadata": {},
   "source": [
    "### Info available for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f773e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The info for the first word is stored in : <class 'dict'>\n",
      "dict_keys(['content', 'n_fixations', 'word_level_EEG'])\n"
     ]
    }
   ],
   "source": [
    "first_word = word_data[0]\n",
    "print('The info for the first word is stored in :', type(first_word))\n",
    "print(first_word.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af9617b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The content of the first word is : Presents\n",
      "The number of fixations on the first word is : 1\n"
     ]
    }
   ],
   "source": [
    "print('The content of the first word is :', first_word['content'])\n",
    "print('The number of fixations on the first word is :', first_word['n_fixations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0a94f9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The EEG features for the first word are stored in : <class 'dict'>\n",
      "dict_keys(['FFD', 'TRT', 'GD'])\n"
     ]
    }
   ],
   "source": [
    "## EEG features for the first word\n",
    "word_level_EEG = first_word['word_level_EEG']\n",
    "print('The EEG features for the first word are stored in :', type(word_level_EEG))\n",
    "print(word_level_EEG.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa6347e",
   "metadata": {},
   "source": [
    "Reminder:\n",
    "\n",
    "**FFD** = The duration of the first fixation on the prevailing word. <br>\n",
    "**TRT** =  The sum of all fixation durations on the current word, including regression (when you go back to the same word and make another fixation that lasts x time). <br>\n",
    "**GD** =  The sum of all fixations on the current word in the first-pass reading before the eyes move out of the word. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9c407528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFD dict_keys(['FFD_t1', 'FFD_t2', 'FFD_a1', 'FFD_a2', 'FFD_b1', 'FFD_b2', 'FFD_g1', 'FFD_g2'])\n",
      "TRT dict_keys(['TRT_t1', 'TRT_t2', 'TRT_a1', 'TRT_a2', 'TRT_b1', 'TRT_b2', 'TRT_g1', 'TRT_g2'])\n",
      "GD dict_keys(['GD_t1', 'GD_t2', 'GD_a1', 'GD_a2', 'GD_b1', 'GD_b2', 'GD_g1', 'GD_g2'])\n"
     ]
    }
   ],
   "source": [
    "for eeg_feature in word_level_EEG.keys():\n",
    "    print(eeg_feature, word_level_EEG[eeg_feature].keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EEG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
